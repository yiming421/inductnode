# wandb sweep configuration
# Optional: give your sweep a descriptive name
# name: my-gnn-pfn-sweep-1

program: pfn.py # TODO: MANDATORY - Change this to your Python script name
method: bayes # Strategy for picking hyperparameters: random, grid, or bayes

project: inductnode
early_terminate: # Optional: configure early stopping for bad runs (Hyperband example)
  type: hyperband
  min_iter: 50 # Minimum number of epochs/steps before a run can be stopped
  s: 2 # Number of brackets
  eta: 3 # Reduction factor

metric:
  name: avg_test_metric # TODO: MANDATORY - Change to your primary metric (e.g., val_loss, test_f1)
  goal: maximize   # TODO: MANDATORY - Change to 'minimize' if a lower value is better

parameters:
  # --- Training & Optimization Hyperparameters ---
  sweep:
    value: True
  epochs:
    values: [100, 200, 300] # Or use distribution: q_uniform, min: 50, max: 500
  lr:
    values: [0.002, 0.001, 0.0005, 0.0001]
  weight_decay:
    values: [0, 1e-6, 1e-4] # Common small values for weight decay
  batch_size: # Training batch size
    values: [2048, 4096, 8192] # Adjust based on your dataset/memory
  optimizer:
    values: ['adam', 'adamw'] # TODO: Ensure your script supports these
  schedule: # Learning rate schedule
    values: ['none', 'cosine', 'step', 'warmup'] # TODO: Replace with actual supported schedule names
  clip_grad:
    values: [0.0, 0.5, 1.0, 2.0] # 0.0 means no gradient clipping

  # --- GNN (e.g., PureGCN_v1) Hyperparameters ---
  model:
    values: ['PureGCN_v1', 'GCN']
  hidden:
    values: [32, 64, 128]
  dp: # Dropout rate for GNN
    values: [0.0, 0.2, 0.4, 0.6]
  num_layers: # Number of GNN layers
    values: [2, 3, 4]
  gnn_norm_affine: # elementwise_affine for GNN LayerNorm
    values: [True, False]
  relu: # Enable/disable ReLU in GNN (if GCN allows this toggle)
    values: [True, False]
  res: # Enable/disable residual connections in GNN
    values: [True, False]
  use_gin: # If GNN can switch to GIN-like behavior
    values: [True, False]
  multilayer: # Meaning depends on your GNN implementation (e.g. for jumping knowledge)
    values: [True, False]

  # --- MLP (e.g., within PFN, or output predictor) Hyperparameters ---
  mlp_layers: # Number of layers in MLP components
    values: [2]
  mlp_norm_affine: # elementwise_affine for MLP LayerNorm (if MLPs have norm)
    values: [True, False]

  # --- Transformer (e.g., within PFN) Hyperparameters ---
  transformer_layers:
    values: [1]
  nhead: # Number of attention heads. Ensure hidden_dim is divisible by nhead.
    values: [1, 4]

  # --- PFN / Other Model-Specific Hyperparameters ---
  seperate: # For PFNTransformerLayer's separate_att
    values: [True, False]
  degree: # Whether to use degree features
    values: [True, False]
  padding:
    values: ['zero', 'mlp'] # TODO: Replace/Adjust with your actual supported padding types
  sim:
    values: ['dot', 'cos', 'mlp'] # TODO: Replace/Adjust with your actual supported similarity types
  att_pool: # Attention-based pooling
    values: [True, False]
  mlp_pool: # MLP-based pooling
    values: [True, False]
  orthogonal_push: # Strength of orthogonal regularization
    values: [0, 1e-6, 1e-4, 1e-2]
  sign_normalize:
    values: [True, False]